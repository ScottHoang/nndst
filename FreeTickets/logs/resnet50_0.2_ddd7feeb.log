22:28:18: Namespace(PF_rate=0.8, batch_size=512, bench=False, data='cifar10', death='magnitude', death_rate=0.5, decay_schedule='constant', density=0.2, epochs=450, epochs_explo=150, fix=False, fp16=False, growth='gradient', iters=1, l1=0.0, l2=0.0005, large_death_rate=0.8, log_interval=100, lr=0.1, max_threads=10, mgpu=False, model='resnet50', model_num=3, momentum=0.9, no_cuda=False, nolrsche=True, optimizer='sgd', redistribution='none', resume=None, save_features=False, seed=17, sparse=True, sparse_init='ERK', start_epoch=1, test_batch_size=128, update_frequency=1000, valid_split=0.1, workers=10, world_size=-1)
22:28:18: 


22:28:18: ================================================================================
22:28:18: 
Iteration start: 1/1

22:28:21: ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=2048, out_features=10, bias=True)
)
22:28:21: ============================================================
22:28:21: resnet50
22:28:21: ============================================================
22:28:21: ============================================================
22:28:21: Prune mode: magnitude
22:28:21: Growth mode: gradient
22:28:21: Redistribution mode: none
22:28:21: ============================================================
22:28:25: Train Epoch: 1 [0/45056 (0%)]	Loss: 2.354725 Accuracy: 38/512 (7.422% 
22:28:37: 
Training summary: Average loss: 3.0445, Accuracy: 4841/45000 (10.758%)

22:28:39: 
Evaluation: Average loss: 2.2958, Accuracy: 515/5000 (10.300%)

22:28:39: Current learning rate: 0.1. Time taken for epoch: 17.17 seconds.

22:28:39: Train Epoch: 2 [0/45056 (0%)]	Loss: 2.354091 Accuracy: 72/512 (14.062% 
22:28:51: 
Training summary: Average loss: 2.1853, Accuracy: 9864/45000 (21.920%)

22:28:52: 
Evaluation: Average loss: 1.9788, Accuracy: 1309/5000 (26.180%)

22:28:52: Current learning rate: 0.1. Time taken for epoch: 12.92 seconds.

22:28:52: Train Epoch: 3 [0/45056 (0%)]	Loss: 1.932860 Accuracy: 142/512 (27.734% 
22:29:04: 
Training summary: Average loss: 1.8028, Accuracy: 15723/45000 (34.940%)

22:29:05: 
Evaluation: Average loss: 1.6606, Accuracy: 1864/5000 (37.280%)

22:29:05: Current learning rate: 0.1. Time taken for epoch: 13.09 seconds.

22:29:05: Train Epoch: 4 [0/45056 (0%)]	Loss: 1.611984 Accuracy: 212/512 (41.406% 
22:29:17: 
Training summary: Average loss: 1.6340, Accuracy: 18521/45000 (41.158%)

22:29:18: 
Evaluation: Average loss: 1.5335, Accuracy: 2146/5000 (42.920%)

22:29:18: Current learning rate: 0.1. Time taken for epoch: 13.24 seconds.

22:29:19: Train Epoch: 5 [0/45056 (0%)]	Loss: 1.523143 Accuracy: 225/512 (43.945% 
22:29:30: 
Training summary: Average loss: 1.5297, Accuracy: 20290/45000 (45.089%)

22:29:31: 
Evaluation: Average loss: 1.4145, Accuracy: 2436/5000 (48.720%)

22:29:31: Current learning rate: 0.1. Time taken for epoch: 13.13 seconds.

22:29:32: Train Epoch: 6 [0/45056 (0%)]	Loss: 1.397760 Accuracy: 241/512 (47.070% 
22:29:43: 
Training summary: Average loss: 1.4312, Accuracy: 21953/45000 (48.784%)

22:29:44: 
Evaluation: Average loss: 1.3706, Accuracy: 2521/5000 (50.420%)

22:29:44: Current learning rate: 0.1. Time taken for epoch: 13.21 seconds.

22:29:45: Train Epoch: 7 [0/45056 (0%)]	Loss: 1.373847 Accuracy: 264/512 (51.562% 
22:29:57: 
Training summary: Average loss: 1.3405, Accuracy: 23436/45000 (52.080%)

22:29:58: 
Evaluation: Average loss: 1.3006, Accuracy: 2623/5000 (52.460%)

22:29:58: Current learning rate: 0.1. Time taken for epoch: 13.40 seconds.

22:29:58: Train Epoch: 8 [0/45056 (0%)]	Loss: 1.317944 Accuracy: 273/512 (53.320% 
22:30:10: 
Training summary: Average loss: 1.2796, Accuracy: 24618/45000 (54.707%)

22:30:11: 
Evaluation: Average loss: 1.2535, Accuracy: 2690/5000 (53.800%)

22:30:11: Current learning rate: 0.1. Time taken for epoch: 13.51 seconds.

22:30:12: Train Epoch: 9 [0/45056 (0%)]	Loss: 1.214576 Accuracy: 300/512 (58.594% 
22:30:24: 
Training summary: Average loss: 1.2006, Accuracy: 25885/45000 (57.522%)

22:30:25: 
Evaluation: Average loss: 1.1433, Accuracy: 2958/5000 (59.160%)

22:30:25: Current learning rate: 0.1. Time taken for epoch: 13.51 seconds.

22:30:25: Train Epoch: 10 [0/45056 (0%)]	Loss: 1.070166 Accuracy: 309/512 (60.352% 
22:30:37: 
Training summary: Average loss: 1.1323, Accuracy: 26929/45000 (59.842%)

22:30:38: 
Evaluation: Average loss: 1.1264, Accuracy: 2995/5000 (59.900%)

22:30:38: Current learning rate: 0.1. Time taken for epoch: 13.49 seconds.

22:30:39: Train Epoch: 11 [0/45056 (0%)]	Loss: 0.990918 Accuracy: 332/512 (64.844% 
22:30:51: 
Training summary: Average loss: 1.0777, Accuracy: 27906/45000 (62.013%)

22:30:52: 
Evaluation: Average loss: 1.0410, Accuracy: 3133/5000 (62.660%)

22:30:52: Current learning rate: 0.1. Time taken for epoch: 13.79 seconds.

22:30:53: Train Epoch: 12 [0/45056 (0%)]	Loss: 0.953831 Accuracy: 345/512 (67.383% 
22:31:05: 
Training summary: Average loss: 1.0916, Accuracy: 27787/45000 (61.749%)

22:31:06: 
Evaluation: Average loss: 1.1466, Accuracy: 2986/5000 (59.720%)

22:31:06: Current learning rate: 0.1. Time taken for epoch: 13.89 seconds.

22:31:07: Train Epoch: 13 [0/45056 (0%)]	Loss: 1.014942 Accuracy: 320/512 (62.500% 
22:31:19: 
Training summary: Average loss: 1.0150, Accuracy: 29048/45000 (64.551%)

22:31:20: 
Evaluation: Average loss: 1.0164, Accuracy: 3196/5000 (63.920%)

22:31:20: Current learning rate: 0.1. Time taken for epoch: 13.57 seconds.

22:31:20: Train Epoch: 14 [0/45056 (0%)]	Loss: 0.960071 Accuracy: 340/512 (66.406% 
22:31:32: 
Training summary: Average loss: 0.9480, Accuracy: 30038/45000 (66.751%)

22:31:34: 
Evaluation: Average loss: 0.9550, Accuracy: 3273/5000 (65.460%)

22:31:34: Current learning rate: 0.1. Time taken for epoch: 13.94 seconds.

22:31:34: Train Epoch: 15 [0/45056 (0%)]	Loss: 0.823507 Accuracy: 372/512 (72.656% 
22:31:46: 
Training summary: Average loss: 0.8972, Accuracy: 30834/45000 (68.520%)

22:31:47: 
Evaluation: Average loss: 0.9823, Accuracy: 3287/5000 (65.740%)

22:31:47: Current learning rate: 0.1. Time taken for epoch: 13.74 seconds.

22:31:48: Train Epoch: 16 [0/45056 (0%)]	Loss: 0.738064 Accuracy: 391/512 (76.367% 
22:32:00: 
Training summary: Average loss: 0.8593, Accuracy: 31569/45000 (70.153%)

22:32:01: 
Evaluation: Average loss: 0.9279, Accuracy: 3358/5000 (67.160%)

22:32:01: Current learning rate: 0.1. Time taken for epoch: 13.94 seconds.

22:32:02: Train Epoch: 17 [0/45056 (0%)]	Loss: 0.768212 Accuracy: 379/512 (74.023% 
22:32:14: 
Training summary: Average loss: 0.8193, Accuracy: 32176/45000 (71.502%)

22:32:15: 
Evaluation: Average loss: 0.9036, Accuracy: 3409/5000 (68.180%)

22:32:15: Current learning rate: 0.1. Time taken for epoch: 13.85 seconds.

22:32:16: Train Epoch: 18 [0/45056 (0%)]	Loss: 0.825431 Accuracy: 368/512 (71.875% 
22:32:28: 
Training summary: Average loss: 0.8006, Accuracy: 32513/45000 (72.251%)

22:32:29: 
Evaluation: Average loss: 0.9204, Accuracy: 3399/5000 (67.980%)

22:32:29: Current learning rate: 0.1. Time taken for epoch: 13.89 seconds.

22:32:30: Train Epoch: 19 [0/45056 (0%)]	Loss: 0.800056 Accuracy: 371/512 (72.461% 
22:32:42: 
Training summary: Average loss: 0.7695, Accuracy: 32966/45000 (73.258%)

22:32:43: 
Evaluation: Average loss: 0.8522, Accuracy: 3507/5000 (70.140%)

22:32:43: Current learning rate: 0.1. Time taken for epoch: 14.07 seconds.

22:32:44: Train Epoch: 20 [0/45056 (0%)]	Loss: 0.667446 Accuracy: 389/512 (75.977% 
22:32:56: 
Training summary: Average loss: 0.7355, Accuracy: 33539/45000 (74.531%)

22:32:57: 
Evaluation: Average loss: 0.8131, Accuracy: 3584/5000 (71.680%)

22:32:57: Current learning rate: 0.1. Time taken for epoch: 13.62 seconds.

22:32:57: Train Epoch: 21 [0/45056 (0%)]	Loss: 0.675902 Accuracy: 396/512 (77.344% 
22:33:09: 
Training summary: Average loss: 0.7304, Accuracy: 33736/45000 (74.969%)

22:33:10: 
Evaluation: Average loss: 0.8392, Accuracy: 3497/5000 (69.940%)

22:33:10: Current learning rate: 0.1. Time taken for epoch: 13.77 seconds.

22:33:11: Train Epoch: 22 [0/45056 (0%)]	Loss: 0.732332 Accuracy: 373/512 (72.852% 
22:33:24: 
Training summary: Average loss: 0.7053, Accuracy: 33989/45000 (75.531%)

22:33:25: 
Evaluation: Average loss: 0.8533, Accuracy: 3532/5000 (70.640%)

22:33:25: Current learning rate: 0.1. Time taken for epoch: 14.26 seconds.

22:33:25: Train Epoch: 23 [0/45056 (0%)]	Loss: 0.761674 Accuracy: 368/512 (71.875% 
22:33:37: 
Training summary: Average loss: 0.7124, Accuracy: 33756/45000 (75.013%)

22:33:38: 
Evaluation: Average loss: 0.8647, Accuracy: 3498/5000 (69.960%)

22:33:38: Current learning rate: 0.1. Time taken for epoch: 13.73 seconds.

22:33:39: Train Epoch: 24 [0/45056 (0%)]	Loss: 0.702612 Accuracy: 377/512 (73.633% 
22:33:51: 
Training summary: Average loss: 0.7316, Accuracy: 33602/45000 (74.671%)

22:33:52: 
Evaluation: Average loss: 0.8338, Accuracy: 3542/5000 (70.840%)

22:33:52: Current learning rate: 0.1. Time taken for epoch: 13.69 seconds.

22:33:53: Train Epoch: 25 [0/45056 (0%)]	Loss: 0.714320 Accuracy: 392/512 (76.562% 
22:34:05: 
Training summary: Average loss: 0.7127, Accuracy: 33940/45000 (75.422%)

22:34:06: 
Evaluation: Average loss: 0.7901, Accuracy: 3618/5000 (72.360%)

22:34:06: Current learning rate: 0.1. Time taken for epoch: 13.75 seconds.

22:34:06: Train Epoch: 26 [0/45056 (0%)]	Loss: 0.620099 Accuracy: 400/512 (78.125% 
